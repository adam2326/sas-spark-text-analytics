{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################################### \n",
    "#  general use\n",
    "############################################################################################### \n",
    "import time\n",
    "from math import pow, floor\n",
    "\n",
    "############################################################################################### \n",
    "#  data access\n",
    "############################################################################################### \n",
    "from google.datalab import Context\n",
    "import google.datalab.storage as storage\n",
    "\n",
    "import csv\n",
    "import urllib2\n",
    "from StringIO import StringIO\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################### \n",
    "#  PySpark - basic\n",
    "############################################################################################### \n",
    "\n",
    "#####https://docs.databricks.com/spark/latest/spark-sql/udf-in-python.html\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import StringType\n",
    "#from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "\n",
    "############################################################################################### \n",
    "#  PySpark - machine learning - http://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html\n",
    "############################################################################################### \n",
    "\n",
    "##### https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.CountVectorizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "##### http://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html#pyspark.ml.clustering.LDA\n",
    "from pyspark.ml.clustering import LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import testing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list your storage buckets\n",
    "for bucket in list(storage.Buckets()):\n",
    "  print bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get project name; just for reference\n",
    "project = Context.default().project_id\n",
    "\n",
    "# select the storage bucket object you want\n",
    "my_bucket = storage.Bucket('machine-learning-backend-storagebucket')\n",
    "for obj in my_bucket.objects():\n",
    "  print(obj.key + '   size: ' + str(obj.metadata.size) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################################################################### \n",
    "#\n",
    "#  two ways to read data in from Google cloud storage into python lists\n",
    "#\n",
    "############################################################################################### \n",
    "# read_stream() - produces a str object\n",
    "d = my_bucket.object('sas_data/airlines.csv').read_stream()\n",
    "data = [row for row in csv.reader( StringIO(d) )]\n",
    "print data[:3]\n",
    "#del(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# urllib2 to read the public link of your gcs object\n",
    "url = 'https://storage.googleapis.com/machine-learning-backend-storagebucket/sas_data/airlines.csv'\n",
    "data = [i for i in csv.reader(StringIO(urllib2.urlopen(url).read()))]\n",
    "print data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark infers all data yptes are string when reading from csv: https://github.com/databricks/spark-csv\n",
      "[('id', 'string'), ('airline', 'string'), ('date', 'string'), ('location', 'string'), ('rating', 'string'), ('cabin', 'string'), ('value', 'string'), ('recommended', 'string'), ('review', 'string')]\n",
      "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+\n",
      "|   id|        airline|     date|location|rating|   cabin|value|recommended|              review|\n",
      "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+\n",
      "|10001|Delta Air Lines|21-Jun-14|Thailand|     7| Economy|    4|        YES|Flew Mar 30 NRT t...|\n",
      "|10002|Delta Air Lines|19-Jun-14|     USA|     0| Economy|    2|         NO|Flight 2463 leavi...|\n",
      "|10003|Delta Air Lines|18-Jun-14|     USA|     0| Economy|    1|         NO|Delta Website fro...|\n",
      "|10004|Delta Air Lines|17-Jun-14|     USA|     9|Business|    4|        YES|\"I just returned ...|\n",
      "|10005|Delta Air Lines|17-Jun-14| Ecuador|     7| Economy|    3|        YES|\"Round-trip fligh...|\n",
      "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################################################################### \n",
    "#\n",
    "#  read data from Google cloud storage into spark\n",
    "#\n",
    "###############################################################################################\n",
    "df = spark.read.csv(\"gs://machine-learning-backend-storagebucket/sas_data/airlines.csv\", header=True)\n",
    "print 'Spark infers all data types are string when reading from csv: https://github.com/databricks/spark-csv'\n",
    "print df.dtypes\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferSchema requires an additional pass over the data: https://github.com/databricks/spark-csv\n",
      "[('id', 'int'), ('airline', 'string'), ('date', 'string'), ('location', 'string'), ('rating', 'int'), ('cabin', 'string'), ('value', 'int'), ('recommended', 'string'), ('review', 'string')]\n",
      "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+\n",
      "|   id|        airline|     date|location|rating|   cabin|value|recommended|              review|\n",
      "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+\n",
      "|10001|Delta Air Lines|21-Jun-14|Thailand|     7| Economy|    4|        YES|Flew Mar 30 NRT t...|\n",
      "|10002|Delta Air Lines|19-Jun-14|     USA|     0| Economy|    2|         NO|Flight 2463 leavi...|\n",
      "|10003|Delta Air Lines|18-Jun-14|     USA|     0| Economy|    1|         NO|Delta Website fro...|\n",
      "|10004|Delta Air Lines|17-Jun-14|     USA|     9|Business|    4|        YES|\"I just returned ...|\n",
      "|10005|Delta Air Lines|17-Jun-14| Ecuador|     7| Economy|    3|        YES|\"Round-trip fligh...|\n",
      "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################################################################### \n",
    "#\n",
    "#  read data from Google cloud storage into spark\n",
    "#\n",
    "###############################################################################################\n",
    "df = spark.read.csv(\"gs://machine-learning-backend-storagebucket/sas_data/airlines.csv\", header=True, inferSchema=True)\n",
    "print 'inferSchema requires an additional pass over the data: https://github.com/databricks/spark-csv'\n",
    "print df.dtypes\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "############################################################################################### \n",
    "#\n",
    "#  start topic analysis\n",
    "#\n",
    "###############################################################################################\n",
    "# calculate number of topics using super secret formula\n",
    "num_topics = int(floor(pow(df.count(), float(1/2.5))))\n",
    "print num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'aboard', 'about', 'above']\n"
     ]
    }
   ],
   "source": [
    "def list_from_stream(bucket_name, bucket_object_name):\n",
    "  my_bucket = storage.Bucket(bucket_name)\n",
    "  d = my_bucket.object(bucket_object_name).read_stream()\n",
    "  strings = [row for row in csv.reader( StringIO(d) )]\n",
    "  return [item for sublist in strings for item in sublist]\n",
    "  \n",
    "# load stopwords from google storage bucket\n",
    "stopwords = list_from_stream('machine-learning-backend-storagebucket', 'sas_data/stop_words.txt')\n",
    "print stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean the text\n",
    "def cleanup_text(record):\n",
    "    text  = record[8]\n",
    "    uid   = record[0]\n",
    "    words = text.split()\n",
    "    # Remove stopwords and words under X length\n",
    "    text_out = [word.lower() for word in words if len(word)>2 and word.lower() not in stopwords]\n",
    "    return text_out\n",
    "\n",
    "# utilize user defined function to clean the text\n",
    "udf_cleantext = udf(cleanup_text , ArrayType(StringType()))\n",
    "clean_text = df.withColumn(\"words\", udf_cleantext(struct([df[x] for x in df.columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+--------------------+\n",
      "|   id|        airline|     date|location|rating|   cabin|value|recommended|              review|               words|\n",
      "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+--------------------+\n",
      "|10001|Delta Air Lines|21-Jun-14|Thailand|     7| Economy|    4|        YES|Flew Mar 30 NRT t...|[flew, mar, nrt, ...|\n",
      "|10002|Delta Air Lines|19-Jun-14|     USA|     0| Economy|    2|         NO|Flight 2463 leavi...|[flight, 2463, le...|\n",
      "|10003|Delta Air Lines|18-Jun-14|     USA|     0| Economy|    1|         NO|Delta Website fro...|[delta, website, ...|\n",
      "|10004|Delta Air Lines|17-Jun-14|     USA|     9|Business|    4|        YES|\"I just returned ...|[returned, round-...|\n",
      "|10005|Delta Air Lines|17-Jun-14| Ecuador|     7| Economy|    3|        YES|\"Round-trip fligh...|[\"round-trip, fli...|\n",
      "+-----+---------------+---------+--------+------+--------+-----+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the data\n",
    "clean_text.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|              review|               words|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|Flew Mar 30 NRT t...|[flew, mar, nrt, ...|(1000,[0,2,11,21,...|(1000,[0,2,11,21,...|\n",
      "|Flight 2463 leavi...|[flight, 2463, le...|(1000,[0,3,6,7,10...|(1000,[0,3,6,7,10...|\n",
      "|Delta Website fro...|[delta, website, ...|(1000,[0,2,4,7,12...|(1000,[0,2,4,7,12...|\n",
      "|\"I just returned ...|[returned, round-...|(1000,[0,1,2,3,7,...|(1000,[0,1,2,3,7,...|\n",
      "|\"Round-trip fligh...|[\"round-trip, fli...|(1000,[0,7,12,16,...|(1000,[0,7,12,16,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency Vectorization\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000, minDF=4)\n",
    "cvmodel = cv.fit(clean_text)\n",
    "featurizedData = cvmodel.transform(clean_text)\n",
    "\n",
    "vocab = cvmodel.vocabulary\n",
    "vocab_broadcast = sc.broadcast(vocab)\n",
    " \n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData) # TFIDF\n",
    "# show the transformed data\n",
    "rescaledData.select('review','words','rawFeatures','features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1000\n",
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[18, 35, 56, 13, ...|[0.01890493052272...|\n",
      "|    1|[141, 39, 248, 68...|[0.01303530958012...|\n",
      "|    2|[106, 48, 217, 64...|[0.01089011198031...|\n",
      "|    3|[143, 94, 66, 291...|[0.02216471707805...|\n",
      "|    4|[7, 671, 620, 569...|[0.01129101223822...|\n",
      "|    5|[84, 25, 13, 29, ...|[0.01197915576566...|\n",
      "|    6|[122, 167, 536, 6...|[0.01708510087273...|\n",
      "|    7|[77, 88, 22, 269,...|[0.01454564644437...|\n",
      "|    8|[410, 276, 23, 73...|[0.01221927595976...|\n",
      "|    9|[193, 15, 214, 24...|[0.01777650397472...|\n",
      "|   10|[2, 28, 464, 9, 9...|[0.00928587923362...|\n",
      "|   11|[287, 5, 205, 8, ...|[0.01677499329144...|\n",
      "|   12|[47, 40, 30, 172,...|[0.02483340607833...|\n",
      "|   13|[115, 134, 6, 150...|[0.01489504302793...|\n",
      "|   14|[62, 249, 7, 486,...|[0.01607533423240...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate 25 Data-Driven Topics:\n",
    "# \"em\" = expectation-maximization \n",
    "lda = LDA(k=num_topics, seed=123, optimizer=\"em\", featuresCol=\"features\")\n",
    "ldamodel = lda.fit(rescaledData)\n",
    " \n",
    "print ldamodel.isDistributed()\n",
    "print ldamodel.vocabSize()\n",
    " \n",
    "ldatopics = ldamodel.describeTopics()\n",
    "# Show Topics\n",
    "ldatopics.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1  Terms: [u'gate', u'connecting', u'later', u'told', u'delayed', u'next']\n",
      "Topic 2  Terms: [u'excellent', u'airways', u'lounge', u'envoy', u'heathrow', u'coach']\n",
      "Topic 3  Terms: [u'help', u'people', u'kept', u'daughter', u'plane', u'delta']\n",
      "Topic 4  Terms: [u'phoenix', u'san', u'southwest', u'online', u'diego', u'upgrade']\n",
      "Topic 5  Terms: [u'delta', u'segments', u'son', u'eat', u'seattle', u'seat']\n",
      "Topic 6  Terms: [u'agent', u'airport', u'told', u'due', u'mechanical', u'canceled']\n",
      "Topic 7  Terms: [u'denver', u'phl', u'representative', u'mexico', u'return', u'carry']\n",
      "Topic 8  Terms: [u'philadelphia', u'aircraft', u'airline', u'charge', u'clt', u'airways']\n",
      "Topic 9  Terms: [u'tokyo', u'personal', u'good', u'friendly', u'terrible', u'delta']\n",
      "Topic 10  Terms: [u'overhead', u'seat', u'sfo', u'price', u'check', u'dtw']\n",
      "Topic 11  Terms: [u'flights', u'boarding', u'boarding.', u'seats', u'2nd', u'plane']\n",
      "Topic 12  Terms: [u'group', u'first', u'informed', u'united', u'lax', u'people']\n",
      "Topic 13  Terms: [u'business', u'economy', u'class', u'ife', u'flat', u'seats']\n",
      "Topic 14  Terms: [u'leaving', u'sat', u'hours', u'charlotte', u'plane', u'received']\n",
      "Topic 15  Terms: [u'air', u'row', u'delta', u'paris', u'food', u'777']\n"
     ]
    }
   ],
   "source": [
    "# generate topic summary\n",
    "topic_summary = list()\n",
    "number_of_terms_per_topic = 6 # max of 10\n",
    "for row in ldatopics.rdd.map(lambda x: x).collect():\n",
    "  topic_id = [row.topic]\n",
    "  topic_indices = row.termIndices[:number_of_terms_per_topic]\n",
    "  topic_weights = row.termWeights\n",
    "  terms = [vocab[idx] for idx in topic_indices]\n",
    "  topic_summary.append( topic_id + terms )\n",
    "\n",
    "for topic in topic_summary:\n",
    "  print \"Topic \" + str(topic[0]+1) + \"  Terms: \" + str(topic[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save and load model\n",
    "ldaModel.save(sc, \"target/org/apache/spark/PythonLatentDirichletAllocationExample/LDAModel\")\n",
    "sameModel = LDAModel.load(sc, \"target/org/apache/spark/PythonLatentDirichletAllocationExample/LDAModel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
